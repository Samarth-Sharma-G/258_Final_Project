{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from mlflow.models.signature import infer_signature\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd \n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.exceptions import UserErrorException    \n",
    "from mlflow.deployments import get_deploy_client\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "from azure.core.exceptions import ResourceNotFoundError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTrainer2:\n",
    "    def __init__(self, pathX, pathY):\n",
    "        self.Xpath = pathX\n",
    "        self.Ypath = pathY\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "        self.champion_model = None\n",
    "        self.champion_model_name = None\n",
    "        self.tracking_tracking_uri = \"azureml://westus2.api.azureml.ms/mlflow/v1.0/subscriptions/7daeeb08-64ac-4094-abc8-02d2a530cc6e/resourceGroups/CMPE258/providers/Microsoft.MachineLearningServices/workspaces/Team-pi\"\n",
    "        # Experiment name will be followed by date time\n",
    "        self.initialization_time = str(pd.Timestamp.now())\n",
    "        self.experiment_name = 'waffer_defect_' + self.initialization_time\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "        # Load the data and perform the train test split\n",
    "        self.load_data_and_perform_split()\n",
    "        mlflow.set_tracking_uri(self.tracking_tracking_uri)\n",
    "        self.register_model_name = 'waffer-defect'\n",
    "\n",
    "    \n",
    "    def load_preprocessed_data(self):\n",
    "        self.X = pd.read_pickle(self.Xpath)\n",
    "        self.Y = pd.read_pickle(self.Ypath)\n",
    "\n",
    "    def perform_train_test_split(self, split=0.2, seed=42):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.Y, test_size=split, random_state=seed)\n",
    "\n",
    "    def model1(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(26, 26, 1)),\n",
    "            tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def model2(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Input(shape=(26, 26, 1)),\n",
    "                tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def model3(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Input(shape=(26, 26, 1)),\n",
    "                tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, model_name, epochs=1):\n",
    "        with mlflow.start_run(run_name=model_name):\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            history = model.fit(self.X_train, self.y_train, epochs=epochs, validation_split=0.1, callbacks=[early_stopping])\n",
    "            loss, accuracy = model.evaluate(self.X_test, self.y_test)\n",
    "            print(f'Test accuracy: {accuracy:.4f}')\n",
    "            \n",
    "            # Log parameters and metrics\n",
    "            mlflow.log_param(\"epochs\", epochs)\n",
    "            mlflow.log_metric(\"loss\", loss)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "            # Log the model\n",
    "            input_example = self.X_train[:1]\n",
    "            output_example = model.predict(input_example)\n",
    "            signature = infer_signature(input_example, output_example)\n",
    "\n",
    "            mlflow.tensorflow.log_model(model, artifact_path=model_name, signature=signature)\n",
    "            # Enable autologging\n",
    "            mlflow.tensorflow.autolog()\n",
    "            \n",
    "        return model, history\n",
    "\n",
    "    def get_champion_model(self):\n",
    "        # Example logic to select the champion model based on accuracy\n",
    "        accuracies = {}\n",
    "        trained_models = {}\n",
    "        models = [self.model1, self.model2, self.model3]\n",
    "        for i, model_fn in enumerate(models, start=1):\n",
    "            model = model_fn()\n",
    "            model_name = f'model{i}'\n",
    "            trained_model, history = self.train_model(model, model_name)\n",
    "            accuracies[f'model{i}'] = max(history.history['val_accuracy'])\n",
    "            trained_models[f'model{i}'] = trained_model\n",
    "        self.champion_model_name = max(accuracies, key=accuracies.get)\n",
    "        self.champion_model = trained_models[self.champion_model_name]\n",
    "        print(f'Champion Model: {self.champion_model_name}')\n",
    "        return self.champion_model\n",
    "\n",
    "    def register_champion_model(self):\n",
    "        # Placeholder for actual registration logic\n",
    "        run_id = mlflow.search_runs(experiment_ids=mlflow.get_experiment_by_name(self.experiment_name).experiment_id)\n",
    "        run_id = run_id[run_id['tags.mlflow.runName'] == self.champion_model_name]['run_id'].values[0]\n",
    "        mlflow.register_model(f\"runs:/{run_id}/{self.champion_model_name}\", self.register_model_name)\n",
    "    \n",
    "    def load_data_and_perform_split(self):\n",
    "        self.load_preprocessed_data()\n",
    "        self.perform_train_test_split()\n",
    "    \n",
    "    def train_register_champion_model(self):\n",
    "        self.get_champion_model()\n",
    "        self.register_champion_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data from the blob storage\n",
    "\n",
    "\n",
    "def get_file_from_blob(filename):\n",
    "    try:\n",
    "        dataset = Dataset.File.from_files(path=(datastore, filename))\n",
    "        dataset.download(target_path='.', overwrite=False)\n",
    "    except UserErrorException as e:\n",
    "        print('File Already Exists in the directory')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        #checking if the endpoint exists\n",
    "        deployment_client.get_endpoint(endpoint_name)\n",
    "        return True\n",
    "    # except mlflow.exceptions.MlflowException as e:\n",
    "    #     print(f\"Error: {e}\")\n",
    "    #     return False\n",
    "    except ResourceNotFoundError:\n",
    "        return False\n",
    "\n",
    "def create_endpoint(endpoint_name,endpoint_config):\n",
    "    #writing the config to a json file so that we can pass it to the create_deployment function\n",
    "    endpoint_config_path = \"endpoint_config.json\"\n",
    "    with open(endpoint_config_path, \"w\") as outfile:\n",
    "        outfile.write(json.dumps(endpoint_config))\n",
    "    endpoint = deployment_client.create_endpoint(\n",
    "        name=endpoint_name,\n",
    "        config={\"endpoint-config-file\": endpoint_config_path},\n",
    "    )\n",
    "\n",
    "def create_deployment(deployment_name, endpoint_name ,deploy_config,model_name,version):\n",
    "    deployment_config_path = \"deployment_config.json\"\n",
    "    with open(deployment_config_path, \"w\") as outfile:\n",
    "        outfile.write(json.dumps(deploy_config))\n",
    "    blue_deployment = deployment_client.create_deployment(\n",
    "        name=deployment_name,\n",
    "        endpoint=endpoint_name,\n",
    "        model_uri=f\"models:/{model_name}/{version}\",\n",
    "        config={\"deploy-config-file\": deployment_config_path},\n",
    "    )\n",
    "\n",
    "def allocate_traffic_to_deployment(traffic_config, endpoint_name,deployment_name):\n",
    "    traffic_config_path = \"traffic_config.json\"\n",
    "    with open(traffic_config_path, \"w\") as outfile:\n",
    "        outfile.write(json.dumps(traffic_config))\n",
    "    #updatin\n",
    "    deployment_client.update_endpoint(\n",
    "        endpoint=endpoint_name,\n",
    "        config={\"endpoint-config-file\": traffic_config_path},\n",
    "    )\n",
    "\n",
    "def get_prod_model_name(endpoint_name):\n",
    "    ep = deployment_client.get_endpoint(endpoint_name)\n",
    "    d_name = list(ep['properties']['traffic'].keys())[0]\n",
    "    print(d_name)\n",
    "    dep = deployment_client.get_deployment(d_name,endpoint=endpoint_name)\n",
    "    model_info = dep['properties']['environmentVariables']['AZUREML_MODEL_DIR'].split('/')\n",
    "    model_version = model_info[-1]# after splitting the last element is the model version\n",
    "    model_name = model_info[-2]# after splitting the second last element is the model name\n",
    "    print(f\"Model Name: {model_name} Model Version: {model_version}\")\n",
    "    return model_name,model_version\n",
    "\n",
    "def load_model_from_registry(model_name, model_version):\n",
    "    # model = client.get_model_version(model_name, model_version)\n",
    "    path = f\"models:/{model_name}/{model_version}\"\n",
    "    model = mlflow.pyfunc.load_model(path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    X = pd.read_pickle(images_extracted_file)\n",
    "    Y = pd.read_pickle(images_labels_file)\n",
    "    return train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "def get_latest_version(model_name):\n",
    "    runs = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    return max([r.version for r in runs])    \n",
    "\n",
    "def comapare_performance_perforamce(model1,model2):\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    y1 = model1.predict(X_test)\n",
    "    y2 = model2.predict(X_test)\n",
    "    #models give out prodability that we need to convert to 0 or 1\n",
    "    y1 = [1 if i > 0.5 else 0 for i in y1]\n",
    "    y2 = [1 if i > 0.5 else 0 for i in y2]\n",
    "    acc1 = accuracy_score(y_test, y1)\n",
    "    acc2 = accuracy_score(y_test, y2)\n",
    "    print(f\"Model 1 Accuracy: {acc1} Model 2 Accuracy: {acc2}\")\n",
    "    if acc1 > acc2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 9ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "Model 1 Accuracy: 0.9269662921348315 Model 2 Accuracy: 0.4887640449438202\n",
      "Prod model is better\n",
      "No need for updation\n"
     ]
    }
   ],
   "source": [
    "#orchestration block \n",
    "if __name__ == '__main__':\n",
    "    #setting up the workspace\n",
    "    subscription_id = '7daeeb08-64ac-4094-abc8-02d2a530cc6e'\n",
    "    resource_group = 'CMPE258'\n",
    "    workspace_name = 'Team-pi'\n",
    "\n",
    "    workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "    datastore = Datastore.get(workspace, \"workspaceblobstore\")\n",
    "\n",
    "    # # Getting the data from the blob storage\n",
    "    images_extracted_file =  'images_extracted.pkl'\n",
    "    images_labels_file = 'images_labels.pkl'\n",
    "    get_file_from_blob(images_extracted_file)\n",
    "    get_file_from_blob(images_labels_file)\n",
    "    \n",
    "    # # Training the model\n",
    "    trainer = CNNTrainer2(images_extracted_file,images_labels_file)\n",
    "    #training a number of model and then registering the champion model\n",
    "    trainer.train_register_champion_model()\n",
    "    \n",
    "    #initializing clients needed for deployment\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    deployment_client = get_deploy_client(mlflow.get_tracking_uri())\n",
    "    endpoint_name = \"team-pi-vtzdu\"\n",
    "    model_name = \"waffer-defect\"\n",
    "    latest_version = get_latest_version(model_name)\n",
    "    endpoint_config = {\n",
    "        \"auth_mode\": \"key\",\n",
    "        \"identity\": {\n",
    "            \"type\": \"system_assigned\"\n",
    "        }\n",
    "        }\n",
    "    blue_deployment_name = \"waffer-defect-1\"\n",
    "    deploy_config = {\n",
    "        \"instance_type\": \"Standard_D2as_v4\",\n",
    "        \"instance_count\": 1,\n",
    "    }\n",
    "    traffic_config = {\"traffic\": {blue_deployment_name: 100}}\n",
    "\n",
    "    if not check_if_endpoint_exists(endpoint_name):\n",
    "        print('Endpoint does not exists Using the latest version to create one')\n",
    "        create_endpoint(endpoint_name,endpoint_config)\n",
    "        create_deployment(blue_deployment_name,endpoint_name,deploy_config,model_name,latest_version)\n",
    "        allocate_traffic_to_deployment(traffic_config,endpoint_name,blue_deployment_name)\n",
    "    else:\n",
    "        # print('deployment exists')\n",
    "        prod_model_name, prod_model_version = get_prod_model_name(endpoint_name)\n",
    "        prod_model = load_model_from_registry(prod_model_name, prod_model_version)\n",
    "        challenger_model = load_model_from_registry(prod_model_name, latest_version)\n",
    "        better_model = comapare_performance_perforamce(prod_model,challenger_model)\n",
    "        if better_model == 2:\n",
    "            #redeply else do nothing\n",
    "            print(\"Challenger model is better\")\n",
    "            new_deployment_name = 'challenger-deployment'\n",
    "            create_deployment(new_deployment_name,endpoint_name,deploy_config,model_name,latest_version)\n",
    "            allocate_traffic_to_deployment(traffic_config,endpoint_name,new_deployment_name)\n",
    "            #delete the old deployment\n",
    "            deployment_client.delete_deployment(blue_deployment_name,endpoint_name)\n",
    "        else:\n",
    "            print(\"Prod model is better\")\n",
    "            print(\"No need for updation\")\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
